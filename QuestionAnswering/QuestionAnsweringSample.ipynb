{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionAnsweringSample.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhM7-n9MD_x_",
        "outputId": "c60e5d7d-fdfe-4f49-9e29-db0378da5514"
      },
      "source": [
        "!pip install --quiet transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 2.1MB 4.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 870kB 19.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 34.4MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S__XBUQZDuVE",
        "outputId": "1cca61f4-2e21-4bee-d3b7-fb2fd4a18ee9"
      },
      "source": [
        "import torch\n",
        "import pprint\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased',return_token_type_ids = True)\n",
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased-distilled-squad')\n",
        "\n",
        "context = \"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\"\n",
        "question = \"What does the 'B' in BERT stand for?\"\n",
        "\n",
        "encoding = tokenizer.encode_plus(question, context)\n",
        "\n",
        "input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "start_scores, end_scores = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask])).values()\n",
        "\n",
        "ans_tokens = input_ids[torch.argmax(start_scores) : torch.argmax(end_scores)+1]\n",
        "answer_tokens = tokenizer.convert_ids_to_tokens(ans_tokens , skip_special_tokens=True)\n",
        "\n",
        "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "print (\"\\nAnswer Tokens: \")\n",
        "print (answer_tokens)\n",
        "\n",
        "answer_tokens_to_string = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "print (\"\\nFinal Answer : \")\n",
        "print (answer_tokens_to_string)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Answer Tokens: \n",
            "['bid', '##ire', '##ction', '##al', 'en', '##code', '##r', 'representations', 'from', 'transformers']\n",
            "\n",
            "Final Answer : \n",
            "bidirectional encoder representations from transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPshDgMCEYH5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}